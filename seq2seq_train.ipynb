{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fELG9vDJuJdc"
   },
   "source": [
    "# seq2seq: Train\n",
    "\n",
    "Quarks To Cosmos with AI Virtual Conference: July 12-16, 2021, Carnegie Mellon University\n",
    "\n",
    "## Contributors\n",
    "\n",
    "Abdulhakim Alnuqaydan, Ali Kadhim, Sergei Gleyzer, Harrison Prosper\n",
    "\n",
    "## Hackathon Contributors\n",
    "\n",
    "Andrew Roberts, Jessica Howard, Samuel Hori, Arvind Balasubramanian, Xiaosheng Zhao, Michael Andrews\n",
    "\n",
    "July 2021\n",
    "\n",
    "## Description\n",
    "\n",
    "Use an encoder/decoder model built using LSTMs to map symbolic mathematical expressions $f(x)$ to their Taylor series expansions to ${\\cal O}(x^5)$.\n",
    "\n",
    "We've heavily borrowed from Charon Guo's excellent tutorial at:\n",
    "\n",
    "https://charon.me/posts/pytorch/pytorch_seq2seq_1/\n",
    "\n",
    "### Model Fitting\n",
    "\n",
    "This notebook performs the following tasks:\n",
    "  1. Read the sequence pairs from __data/seq2seq_data_10000.txt__.\n",
    "  1. Convert the sequence pairs to pairs of integer-coded arrays.\n",
    "  1. Create dataloaders for the train, validation, and test sets.\n",
    "  1. Implement the sequence to sequence (seq2seq) model.\n",
    "  1. Fit the model to the data, taking care to save the best model as determined by the validation score,  and compute the cross entropy on the full test set. \n",
    "  \n",
    "Notes:\n",
    "  1. We tokenize down to the level of characters, that is, each character is a token. However, it may be more sensible to keep words such as $\\sin$, $\\cos$, etc., as single tokens.\n",
    "  1. We do not pad the sequences, consequently, the cross entropy calculation is quite noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LFL9-eGV-nV"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfyAPbvu4l3f"
   },
   "source": [
    "### Using Google Colaboratory\n",
    "If you want to use Google colab then uncomment the instructions in the next cell. When that cell is executed, you'll be directed to a Google sign-in page. Once signed in, copy the validation code into the entry window on this page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QF9IVbkY-rJF"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24393,
     "status": "ok",
     "timestamp": 1626528208202,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "qTWYqrekuJde",
    "outputId": "63dcde03-96dd-4b1f-a567-3e2c052eaef5"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive \n",
    "#drive.mount('/content/gdrive')\n",
    "#import sys\n",
    "#sys.path.append('/content/gdrive/My Drive/AI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5240,
     "status": "ok",
     "timestamp": 1626528217716,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "GwFXx5YluJde"
   },
   "outputs": [],
   "source": [
    "# symbolic mathematics\n",
    "import sympy as sp\n",
    "from sympy import exp, \\\n",
    "    cos, sin, tan, \\\n",
    "    cosh, sinh, tanh, ln, log, E\n",
    "\n",
    "x = sp.Symbol('x')\n",
    "\n",
    "# array manipulation\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import math\n",
    "import time\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "# custom data prep and loader\n",
    "from seq2sequtil import loadData, \\\n",
    "Seq2SeqDataPreparer, Seq2SeqDataLoader\n",
    "\n",
    "# enable pretty printing of symbolic equations\n",
    "from IPython.display import display\n",
    "sp.init_printing(use_latex='mathjax')\n",
    "\n",
    "# draw graphs inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1626528226349,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "EvmoUxfYuJdf"
   },
   "outputs": [],
   "source": [
    "#BASE = '/content/gdrive/My Drive/AI'\n",
    "BASE = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rfrZXDE4l3n"
   },
   "source": [
    "Duration of an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1626528229158,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "xOUoa7pT4l3n"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjoD0FFjEHSc"
   },
   "source": [
    "Function to count the number of parameters in a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1626528230844,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "riLbeY85EF5r"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gur9S9MWuJdi"
   },
   "source": [
    "### Load sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "executionInfo": {
     "elapsed": 1532,
     "status": "ok",
     "timestamp": 1626528233892,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "TvB9KSRA4l3h",
    "outputId": "06488a89-1975-4916-c270-bd6c67157ac0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example source:\n",
      "\t(3*x**2-3)*sin(-6*x+2)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - \\left(3 x^{2} - 3\\right) \\sin{\\left(6 x - 2 \\right)}$"
      ],
      "text/plain": [
       " ⎛   2    ⎞             \n",
       "-⎝3⋅x  - 3⎠⋅sin(6⋅x - 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example target:\n",
      "\t-3*sin(2)+18*x*cos(2)+57*x**2*sin(2)-126*x**3*cos(2)-216*x**4*sin(2)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 216 x^{4} \\sin{\\left(2 \\right)} - 126 x^{3} \\cos{\\left(2 \\right)} + 57 x^{2} \\sin{\\left(2 \\right)} + 18 x \\cos{\\left(2 \\right)} - 3 \\sin{\\left(2 \\right)}$"
      ],
      "text/plain": [
       "       4               3              2                                \n",
       "- 216⋅x ⋅sin(2) - 126⋅x ⋅cos(2) + 57⋅x ⋅sin(2) + 18⋅x⋅cos(2) - 3⋅sin(2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs, targets = loadData('%s/data/seq2seq_data_10000.txt' % BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGqD3UHMVtyU"
   },
   "source": [
    "### Convert sequence data\n",
    "  1. Scan input and output sequences and construct maps of characters (tokens) to indices, one map for input sequences and another for target sequences.\n",
    "  1. Pad sequences to the same length\n",
    "  1. Split into train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 593,
     "status": "ok",
     "timestamp": 1626528240728,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "-WrE8fiNVtyV",
    "outputId": "24cd22ee-77df-44a1-d2e4-f37db7d3dc48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of seq-pairs (train):     8000\n",
      "number of seq-pairs (valid):     1000\n",
      "number of seq-pairs (test):      1000\n",
      "\n",
      "number of source tokens:           31\n",
      "max source sequence length:        81\n",
      "\n",
      "number of target tokens:           35\n",
      "max target sequence length:       883\n"
     ]
    }
   ],
   "source": [
    "import seq2sequtil as sq\n",
    "import importlib \n",
    "importlib.reload(sq)\n",
    "\n",
    "fractions=[8/10, 9/10]\n",
    "\n",
    "db = sq.Seq2SeqDataPreparer(inputs, targets, fractions)\n",
    "\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47irOA8H4l3n"
   },
   "source": [
    "Note: The class __Seq2SeqDataLoader__ is a custom dataloader that returns batches of sequence pairs comprising sources and targets, X and Y, respectively. The quantities X and Y are 2D tensors, each with shape (max_seq_len, batch_size) that comprise sequences of indices arranged in columns. Each index corresponds to a token, i.e., a character. The column lengths, max_seq_len, differ between X and Y.\n",
    "\n",
    "\n",
    "__NB:__ Remember to initialize the dataloader (really a data sampler) using its __init_(num_samples, sample)__ method (formally known as reset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1626532749780,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "al20iuoOJljr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE  = torch.device(\"cuda\" \\\n",
    "                           if torch.cuda.is_available() \\\n",
    "                           else \"cpu\")\n",
    "print('DEVICE:', DEVICE)\n",
    "\n",
    "train_dataloader = Seq2SeqDataLoader(db.train_data, device=DEVICE)\n",
    "valid_dataloader = Seq2SeqDataLoader(db.valid_data, device=DEVICE)\n",
    "test_dataloader  = Seq2SeqDataLoader(db.test_data,  device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjCtDW324l3i"
   },
   "source": [
    "## Implement encoder\n",
    "\n",
    "### LSTM \n",
    "\n",
    "An LSTM is a function that is typically conceptualized as a \"device\" containing various \"gates\" that filter input data in different ways. This creative conceptual reasoning has yielded functions with amazing capabilities. However, it is far from clear that this approach will be the way forward in the future. Why? Because we have compelling evidence that highly creative conceptual reasoning, while impressive, is not, actually, needed to arrive at functions with immense capability. The existence of human brains that have evolved through natural selection is an existence proof that immensely capable functions can be arrived at through trial and error.\n",
    "\n",
    "No doubt, one day, someone will devise an effective evolutionary algorithm that will compress millions of years of evolution into a matter of weeks or even days in order to construct immensely capable functions that could far outstrip what could be done through reasoning. \n",
    "\n",
    "#### LSTM Function\n",
    "\n",
    "At time step $t$, instances of the LSTM class in PyTorch compute the function\n",
    "\n",
    "\\begin{align*}\n",
    " g_t & = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}),\\\\\n",
    " i_t & = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}),\\\\\n",
    " f_t & = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}),\\\\ \\\\\n",
    " o_t & = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}),\\\\\n",
    " c_t & = f_t \\odot c_{t-1} + g_t \\odot i_t,\\\\\n",
    " h_t & = o_t \\odot \\tanh(c_t),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\sigma$ is a sigmoid and $\\odot$ is the Hadamard product. The functions $i_t$, and $f_t$ are called the *input* and *forget* gates, respectively, while $c_t$ and $h_t$ are called the *cell* and *hidden* states, respectively, of the LSTM. The cell and hidden states $c_{t-1}$ and $h_{t-1}$ are from the previous time step. The number of elements that comprise the cell and hidden states is specified by the *hidden_size* argument of the PyTorch LSTM. The $W$s and $b$s are the parameters of the LSTM function. The LSTM outputs $o_t, (h_t, c_t)$.\n",
    "\n",
    "### PyTorch LSTM Parameters\n",
    "\n",
    "* input_size – The number of expected features in the input x\n",
    "* hidden_size – The number of features in the hidden state h\n",
    "* num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1\n",
    "* bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "* batch_first – If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False\n",
    "* dropout – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "* bidirectional – If True, becomes a bidirectional LSTM. Default: False\n",
    "* proj_size – If > 0, will use LSTM with projections of corresponding size. Default: 0\n",
    "\n",
    "### Encoder\n",
    "  * num_features - Number of unique source tokens\n",
    "  * embed_size - Dimension of embedding space for tokens\n",
    "  * hidden_size - See above\n",
    "  * number_layers - See above\n",
    "  * dropout - See above\n",
    "  * device - device on which to do calculations\n",
    "  \n",
    "### Decoder\n",
    "  * num_features - Number of target tokens\n",
    "  * embed_size - Dimension of embedding space for tokens\n",
    "  * hidden_size - See above\n",
    "  * number_layers - See above\n",
    "  * dropout - See above\n",
    "  * device - device on which to do calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1626532921305,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "JosdN5LlVtyW",
    "outputId": "3e2221d6-4cef-4814-d5ff-31482ef71eeb"
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 64        # size of vectors holding hidden and cell states\n",
    "NUM_LAYERS  =  4        # number of LSTM layers\n",
    "\n",
    "ENCODER_EMBED_SIZE = 10 # dimension of token embedding space for encoder\n",
    "ENCODER_DROPOUT    = 0.5# probability to zero out a node\n",
    "\n",
    "DECODER_EMBED_SIZE = 10 # dimension of token embedding space for decoder\n",
    "DECODER_DROPOUT    = 0.5# probability to zero out a node\n",
    "\n",
    "CLIP    =   1   # prevent gradients from exploding!\n",
    "N_EPOCHS= 500   # default number of epochs\n",
    "\n",
    "SOS     = db.y_token2index['\\t'] # Start-Of-Sequence\n",
    "EOS     = db.y_token2index['\\n'] # End-Of-Sequence \n",
    "PADDING = db.y_token2index[' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 221,
     "status": "ok",
     "timestamp": 1626530579060,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "JdD6YxTN4l3j"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_features, \n",
    "                 embed_size=ENCODER_EMBED_SIZE, \n",
    "                 hidden_size=HIDDEN_SIZE, \n",
    "                 num_layers=NUM_LAYERS, \n",
    "                 dropout=ENCODER_DROPOUT,\n",
    "                 device=DEVICE):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.num_features= num_features\n",
    "        self.embed_size  = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "        self.dropout     = dropout\n",
    "        \n",
    "        # The embedding will map each index associated with a token \n",
    "        # into a vector of dimension embed_size, where in a typical\n",
    "        # natural language translation task embed_size << num_features.\n",
    "        # However, in this example that is not the case.\n",
    "        #\n",
    "        # The shape of the inputs to the embedding object is \n",
    "        #  (sequence_length, batch_size).\n",
    "        #\n",
    "        # The shape of the outputs is \n",
    "        #  (sequence_length, batch_size, embed_size)\n",
    "        self.embedding = nn.Embedding(num_features, \n",
    "                                      embed_size).to(device)\n",
    "        \n",
    "        # By default, LSTM expects the input shape to be\n",
    "        #  (seq_len, batch_size, embed_size) (seq_len: sequence_length)\n",
    "        self.lstm = nn.LSTM(embed_size, \n",
    "                            hidden_size, \n",
    "                            num_layers,\n",
    "                            dropout=dropout).to(device)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape: (seq_len, batch_size)\n",
    "       \n",
    "        x = self.embedding(x)\n",
    "        # x.shape: (seq_len, batch_size, embed_size)\n",
    "        \n",
    "        output, (hidden, cell) = self.lstm(x)\n",
    "        # output.shape: (seq_len, batch_size, hidden_size)\n",
    "\n",
    "        # we discard output of encoder\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6SStaRM4l3j"
   },
   "source": [
    "## Implement Decoder\n",
    "\n",
    "The decoder is similar to the encoder in that it has an embedding layer that maps the target features (the indices associated with the characters in the target sequences) to a vector in an embedding space. \n",
    "\n",
    "The key difference is that the output of the LSTM is passed to a *Linear* layer that outputs a vector equal in size to the number of features, i.e., unique tokens associated with the target sequences. \n",
    "\n",
    "Starting with the start of sequence (SOS) token (here a tab), the decoder outputs a vector of floats each corresponding to a target token. The ordinal value of the largest float is the decoder's prediction of the next token in the target sequence. The prediction forms the input token for the next call to the decoder. The decoder is called repeatedly until it predicts an end of sequence (EOS) token (here a newline) or the maximum target sequence length is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1626528265920,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "uFyw9rHu4l3j"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    num_features number of target features, i.e., tokens\n",
    "    '''\n",
    "    def __init__(self, num_features, \n",
    "                 embed_size=ENCODER_EMBED_SIZE, \n",
    "                 hidden_size=HIDDEN_SIZE, \n",
    "                 num_layers=NUM_LAYERS, \n",
    "                 dropout=ENCODER_DROPOUT,\n",
    "                 device=DEVICE):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.num_features= num_features  # number of target tokens\n",
    "        self.embed_size  = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "        self.dropout     = dropout\n",
    "        self.device      = device\n",
    " \n",
    "        # The shape of inputs must be (seq_len, batch_size)\n",
    "        # The shape of outputs is (seq_len, batch_size, embed_size)\n",
    "        self.embedding   = nn.Embedding(num_features, \n",
    "                                        embed_size).to(device)\n",
    "\n",
    "        # inputs.shape:  (seq_len, batch_size, embed_size)\n",
    "        # outputs.shape: (seq_len, batch_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size  = embed_size, \n",
    "                            hidden_size = hidden_size, \n",
    "                            num_layers  = num_layers,\n",
    "                            dropout     = dropout).to(device)\n",
    "\n",
    "        # inputs.shape:  (batch_size, hidden_size)\n",
    "        # outputs.shape: (batch_size, num_features)\n",
    "        self.linear = nn.Linear(hidden_size, num_features).to(device)\n",
    "\n",
    "    def forward(self, source, hidden, cell):\n",
    "        # For a given batch instance, the indices of tokens will be \n",
    "        # passed one by one to this function. Therefore, the source\n",
    "        # is a 1D tensor of token indices of shape (batch_size).\n",
    "        # But since the embedding object requires an input with shape \n",
    "        # (seq_len, batch_size) we need to use unsqueeze(0) to \n",
    "        # insert an extra dimension of size 1 at dim=0 so that the\n",
    "        # shape is (1, batch_size).\n",
    "        \n",
    "        source = source.unsqueeze(0)\n",
    "        # source.shape:   (1, batch_size)\n",
    "        \n",
    "        # Map each index in the batch of indices to its embedded \n",
    "        # representation.\n",
    "        embedded = self.embedding(source)\n",
    "        # embedded.shape: (1, batch_size, embed_size)\n",
    "                \n",
    "        # Input embedded representation of token and previous\n",
    "        # hidden and cell states to lstm\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        # output.shape:   (1, batch_size, hidden_size)\n",
    "        \n",
    "        # Get rid of dim 0 of length \"1\" to match shape expected by\n",
    "        # the linear layer\n",
    "        output = output.squeeze(0) \n",
    "        # output.shape:   (batch_size, hidden_size)\n",
    "        \n",
    "        prediction = self.linear(output)\n",
    "        # prediction.shape: (batch_size, num_features)\n",
    "        \n",
    "        # We don't need to apply a softmax to prediction because \n",
    "        # this is done by the PyTorch cross entropy class. \n",
    "        # However, we may want to do so at some point, if we write\n",
    "        # our own custom average loss function.\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jhwo7q1Z4l3k"
   },
   "source": [
    "### Construct seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1626531329640,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "Q35uffAu4l3k"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    '''\n",
    "    model = Model(encoder, decoder, device)\n",
    "    '''\n",
    "    def __init__(self, encoder, decoder, device, teacher_prob=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device  = device\n",
    "        self.teacher_prob = teacher_prob\n",
    "        self.num_features = self.decoder.num_features\n",
    "        \n",
    "        assert encoder.hidden_size == decoder.hidden_size, \\\n",
    "            \"hidden_size of encoder and decoder must be equal!\"\n",
    "        \n",
    "        assert encoder.num_layers == decoder.num_layers, \\\n",
    "            \"num_layers of encoder and decoder must be equal!\"\n",
    "        \n",
    "    def forward(self, source, target):\n",
    "        '''\n",
    "    1.  If target is a tuple, then it is interpreted as:\n",
    "        target = (target, teacher_prob)\n",
    "            \n",
    "    2.  If the target is not a tuple, it is assumed to be the target\n",
    "        and the teacher_prob is set to 0.5\n",
    "        '''\n",
    "\n",
    "        if type(target) == type(()):\n",
    "            target, teacher_prob = target\n",
    "        else:\n",
    "            teacher_prob = self.teacher_prob\n",
    "            \n",
    "        y_seq_len, batch_size  = target.shape\n",
    "            \n",
    "        # source.shape = (x_seq_len, batch_size)\n",
    "        # target.shape = (y_seq_len, batch_size)\n",
    "        #\n",
    "        # teacher_prob is the probability, during training, to \n",
    "        # use the true target rather than the predicted target.\n",
    "        # ideally that probability should be gradually reduced \n",
    "        # as the training progresses.\n",
    "           \n",
    "        # Tensor to store decoder outputs\n",
    "        outputs = torch.zeros(y_seq_len, \n",
    "                              batch_size, \n",
    "                              self.num_features, \n",
    "                              device=self.device)\n",
    "        \n",
    "        # Use last hidden state of the encoder as the initial \n",
    "        # hidden state of the decoder. (Note: the encoder discards\n",
    "        # the output of the LSTM.)\n",
    "        hidden, cell = self.encoder(source)\n",
    "        \n",
    "        # Repeatedly call decoder and have it predict which target\n",
    "        # token should come next.\n",
    "        #\n",
    "        # The first input to the decoder should be the index \n",
    "        # associated with the tab token of the target sequence\n",
    "        index = target[0,:]\n",
    "        \n",
    "        for t in range(1, y_seq_len):\n",
    "            \n",
    "            output, hidden, cell = self.decoder(index, hidden, cell)\n",
    "            \n",
    "            # Cache predictions for each token\n",
    "            # output.shape: (batch_size, num_features)\n",
    "            # Note: following does a deepcopy of output to outputs[t]\n",
    "            outputs[t] = output\n",
    "                       \n",
    "            # The prediction is the ordinal value of the feature with \n",
    "            # the largest value\n",
    "            prediction = output.argmax(1) \n",
    "            \n",
    "            # For the next index, decide whether to use the \n",
    "            # target or the prediction, that is, whether to ask the\n",
    "            # teacher for help \n",
    "            if self.training:\n",
    "                use_target = rn.random() < teacher_prob\n",
    "                index = target[t] if use_target else prediction\n",
    "            else:\n",
    "                index = prediction   \n",
    "                            \n",
    "        # Note: For a given batch instance, the num_features outputs\n",
    "        # do not sum to unity.\n",
    "        return outputs\n",
    "    \n",
    "    def predict(self, source, y_max_seq_len=1000):\n",
    "        # source.shape = (x_seq_len)\n",
    "        \n",
    "        sos = source[0]   # start of sequence code\n",
    "        eos = source[-1]  # end of sequence code\n",
    "        \n",
    "        # Tensor to store decoder predictions\n",
    "        predictions = torch.zeros(y_max_seq_len,\n",
    "                                  dtype=torch.long,\n",
    "                                  device=self.device)\n",
    "        \n",
    "        # Use last hidden state of the encoder as the initial \n",
    "        # hidden state of the decoder.\n",
    "        \n",
    "        # encoder expects input of shape (seq_len, batch_size), so\n",
    "        # we need to unsqueeze, that is, expand into the batch\n",
    "        # dimension\n",
    "        source = source.unsqueeze(1)\n",
    "        # source.shape = (x_seq_len, 1)\n",
    "        \n",
    "        hidden, cell = self.encoder(source)\n",
    "        \n",
    "        # Repeatedly call decoder to get predicted target sequence.\n",
    "        # The first input to the decoder should be the index \n",
    "        # associated with the tab token. Here we assume that the\n",
    "        # source and target tab and newline codes are the same!\n",
    "        index = source[0, :]\n",
    "        count = 0\n",
    "        \n",
    "        for t in range(1, y_max_seq_len):\n",
    "            count += 1\n",
    "            \n",
    "            output, hidden, cell = self.decoder(index, hidden, cell)\n",
    "            # output.shape: (1, num_features)\n",
    "            \n",
    "            # Predict next token by returning the ordinal value\n",
    "            # of the output \"feature\" with the largest value\n",
    "            index = output.argmax(1) \n",
    "            if index == eos:\n",
    "                break\n",
    "                \n",
    "            # cache prediction\n",
    "            predictions[t-1] = index\n",
    "        \n",
    "        # send results to CPU\n",
    "        return predictions[:count].cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yD5JTRC4l3k"
   },
   "source": [
    "### Train Model\n",
    "\n",
    "  1. Create an encoder\n",
    "  1. Create a decoder\n",
    "  1. Create a encoder/decoder model\n",
    "  1. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1626528517045,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "LZ0sDFXH4l3l",
    "outputId": "259174f4-2659-4ba6-b1bb-fc6c1acb4dae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(31, 10)\n",
      "    (lstm): LSTM(10, 64, num_layers=4, dropout=0.5)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(35, 10)\n",
      "    (lstm): LSTM(10, 64, num_layers=4, dropout=0.5)\n",
      "    (linear): Linear(in_features=64, out_features=35, bias=True)\n",
      "  )\n",
      ")\n",
      "Computational device:      cuda\n",
      "Number of free parameters: 241527\n"
     ]
    }
   ],
   "source": [
    "num_source_features = db.num_tokens('source')\n",
    "\n",
    "encoder = Encoder(num_source_features, \n",
    "                  ENCODER_EMBED_SIZE, \n",
    "                  HIDDEN_SIZE, \n",
    "                  NUM_LAYERS, \n",
    "                  ENCODER_DROPOUT).to(DEVICE)\n",
    "\n",
    "num_target_features = db.num_tokens('target')\n",
    "\n",
    "decoder = Decoder(num_target_features, \n",
    "                  DECODER_EMBED_SIZE, \n",
    "                  HIDDEN_SIZE, \n",
    "                  NUM_LAYERS, \n",
    "                  DECODER_DROPOUT).to(DEVICE)\n",
    "\n",
    "model = Model(encoder, decoder, DEVICE).to(DEVICE)\n",
    "\n",
    "print(model)\n",
    "print('Computational device:      %s' % DEVICE)\n",
    "NUM_PARAMETERS = count_parameters(model)\n",
    "print('Number of free parameters: %d' % NUM_PARAMETERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nv6qIvR34l3l"
   },
   "source": [
    "### Choose optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 208,
     "status": "ok",
     "timestamp": 1626528521061,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "d1eEV5Am4l3l"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FJGemwi4l3l"
   },
   "source": [
    "### Choose loss function\n",
    "For each token with associated index k, compute the cross-entropy loss\n",
    "\n",
    "\\begin{align*}\n",
    "    E_k & = -\\log \\left( \\frac{\\exp(\\hat{y}_k)}{\\sum_{\\{j\\}} \\exp(\\hat{y}_j) } \\right) ,\n",
    "\\end{align*}\n",
    "\n",
    "where $\\hat{y}_j$ is the $j^\\text{th}$ element of the model's output vector of length *num_features*. The losses $E_k$ are averaged over all tokens in a sequence and all batch instances. The set $\\{ j \\}$  excludes tokens that correspond to padding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1626528523975,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "C0HLgdIl4l3l"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=PADDING).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXchIB4x4l3l"
   },
   "source": [
    "### Define trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1626532634566,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "Uu0qrPWX4l3m"
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device, \n",
    "          clip, num_samples=400):\n",
    "    \n",
    "    # set to train mode\n",
    "    model.train()  \n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # X and Y are created directly on the device, \n",
    "    # which is most likely a GPU\n",
    "\n",
    "    dataloader.init(num_samples)\n",
    "\n",
    "    for i, (X, Y) in enumerate(dataloader):\n",
    "        \n",
    "        # zero gradients of all trainable parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute output of model\n",
    "        output = model(X, Y)\n",
    "        # output.shape: (y_seq_len, batch_size, num_features)\n",
    "    \n",
    "        # skip first token (a tab), then reshape to\n",
    "        # ((y_seq_len-1)*batch_size, num_features)\n",
    "        num_features = output.shape[-1]\n",
    "        output = output[1:].view(-1, num_features)\n",
    "\n",
    "        # skip first token (a tab), then reshape to\n",
    "        # ((max_target_seq_len-1)*batch_size)\n",
    "        Y = Y[1:].view(-1)\n",
    "               \n",
    "        # average loss over batches\n",
    "        loss = criterion(output, Y)\n",
    "        \n",
    "        # compute gradients using automatic differentiation\n",
    "        loss.backward()\n",
    "        \n",
    "        # clip gradients so they don't blow up\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        # make one step in th model parameter space\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() # to CPU, if using a GPU\n",
    "\n",
    "        #if i % 10 == 0:\n",
    "        #    print(' %d' % i, end='')\n",
    "            \n",
    "    #print(' end', end='')\n",
    "    return epoch_loss / (i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voIKn--S4l3m"
   },
   "source": [
    "### Evaluator\n",
    "\n",
    "In evaluation mode, we turn off gradient calculation and we provide only the source sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1626528532468,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "03jnoaBr4l3m"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device, \n",
    "             num_samples=400, sample=True):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "   \n",
    "    count_correct = 0\n",
    "    char_correct = 0.\n",
    "    count_total = 0\n",
    "    count_char = 0\n",
    "\n",
    "    # no need to compute gradients\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        dataloader.init(num_samples, sample)\n",
    "        \n",
    "        for i, (X, Y) in enumerate(dataloader):\n",
    "                        \n",
    "            output = model(X, Y)\n",
    "            # output.shape:\n",
    "            # (y_seq_len, batch_size, num_features)\n",
    "\n",
    "            # skip first token (a tab), then reshape to\n",
    "            # ((y_seq_len-1)*batch_size, num_features)\n",
    "            \n",
    "            #print(i, list(output.size()), list(Y.size()))\n",
    "            for j in range(len(output[0])):\n",
    "                outstr = torch.argmax(output[1:,j],dim=1)\n",
    "                #print(list(outstr.size()),list(Y[1:,j].size()))\n",
    "                eq_tnsr = torch.eq(outstr, Y[1:,j])\n",
    "                if torch.all(eq_tnsr):\n",
    "                    #print(outstr, Y[1:,j])\n",
    "                    count_correct += 1\n",
    "                count_total += 1\n",
    "                count_char += len(eq_tnsr)\n",
    "                char_correct += float(torch.sum(eq_tnsr)) \n",
    "            \n",
    "            num_features = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, num_features)\n",
    "                            \n",
    "            # skip first token (a tab), then reshape targets to\n",
    "            # ((y_seq_len-1)*batch_size)\n",
    "            Y = Y[1:].view(-1)\n",
    "               \n",
    "            # average loss over mini-batches, excluding padding tokens\n",
    "            loss = criterion(output, Y)\n",
    "        \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        print(\"\\n\\nEvaluation diagnostics:\\nexpressions correct: %d\\nchars correct per expression: %f\\nchar correct rate: %f\\nchars per expression: %f\\n\" % \n",
    "              (count_correct,char_correct/count_total,char_correct/count_char,count_char/count_total))\n",
    "            \n",
    "    return epoch_loss / (i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1626516707129,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "Cf9H6D73Vtya"
   },
   "outputs": [],
   "source": [
    "def train_me_record_loss(model_file, loss_file,\n",
    "                         n_epochs=N_EPOCHS,  \n",
    "                         device=DEVICE,\n",
    "                         clip=CLIP):\n",
    "  \n",
    "    best_valid_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "    \n",
    "        start_time = time.time()\n",
    "    \n",
    "        train_loss = train(model, train_dataloader, optimizer, \n",
    "                           criterion, device, clip)\n",
    "        \n",
    "        valid_loss = evaluate(model, valid_dataloader, \n",
    "                              criterion, device)\n",
    "\n",
    "        end_time   = time.time()\n",
    "        mins, secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "        if valid_loss < best_valid_loss:\n",
    "            # save best model so far\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), model_file)\n",
    "    \n",
    "        print('\\repoch: %5d|%2.2d:%2.2d| train_loss: %7.4f |'\\\n",
    "              ' valid_loss: %7.4f' % \\\n",
    "              (epoch, mins, secs, train_loss, valid_loss), end='')\n",
    "        \n",
    "        # open file in append mode\n",
    "        open(loss_file, 'a').write('%7.4f %7.4f\\n' % (train_loss, \n",
    "                                                      valid_loss))\n",
    "        \n",
    "    print('\\ndone!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1626528573776,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "saGQrywTqFdt"
   },
   "outputs": [],
   "source": [
    "model_count = 1\n",
    "model_file  = '%s/seq2seq_model_%2.2d.pth'  % (BASE, model_count)\n",
    "loss_file   = '%s/seq2seq_losses_%2.2d.txt' % (BASE, model_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AzvgGUMWVtya",
    "outputId": "479ff094-f307-4001-ad57-07d625bcfee9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Evaluation diagnostics:\n",
      "expressions correct: 10\n",
      "chars correct per expression: 19.525843\n",
      "char correct rate: 0.217693\n",
      "chars per expression: 89.694382\n",
      "\n",
      "epoch:     0|00:59| train_loss:  2.5182 | valid_loss:  2.4613\n",
      "\n",
      "Evaluation diagnostics:\n",
      "expressions correct: 2\n",
      "chars correct per expression: 21.062500\n",
      "char correct rate: 0.219623\n",
      "chars per expression: 95.903017\n",
      "\n",
      "epoch:     1|01:03| train_loss:  2.5350 | valid_loss:  2.4668\n",
      "\n",
      "Evaluation diagnostics:\n",
      "expressions correct: 3\n",
      "chars correct per expression: 21.324385\n",
      "char correct rate: 0.208838\n",
      "chars per expression: 102.109620\n",
      "\n",
      "epoch:     2|01:02| train_loss:  2.5504 | valid_loss:  2.5143\n",
      "\n",
      "Evaluation diagnostics:\n",
      "expressions correct: 4\n",
      "chars correct per expression: 20.410367\n",
      "char correct rate: 0.219482\n",
      "chars per expression: 92.993521\n",
      "\n",
      "epoch:     3|00:56| train_loss:  2.4815 | valid_loss:  2.4430\n",
      "\n",
      "Evaluation diagnostics:\n",
      "expressions correct: 2\n",
      "chars correct per expression: 19.364865\n",
      "char correct rate: 0.216623\n",
      "chars per expression: 89.394144\n",
      "\n",
      "epoch:     4|00:59| train_loss:  2.5375 | valid_loss:  2.4790\n",
      "\n",
      "Evaluation diagnostics:\n",
      "expressions correct: 2\n",
      "chars correct per expression: 20.178723\n",
      "char correct rate: 0.221568\n",
      "chars per expression: 91.072340\n",
      "\n",
      "epoch:     5|01:00| train_loss:  2.5300 | valid_loss:  2.4203\n",
      "\n",
      "Evaluation diagnostics:\n",
      "expressions correct: 3\n",
      "chars correct per expression: 20.034783\n",
      "char correct rate: 0.218731\n",
      "chars per expression: 91.595652\n",
      "\n",
      "epoch:     6|01:01| train_loss:  2.5316 | valid_loss:  2.4529\n",
      "\n",
      "Evaluation diagnostics:\n",
      "expressions correct: 3\n",
      "chars correct per expression: 20.552795\n",
      "char correct rate: 0.218205\n",
      "chars per expression: 94.190476\n",
      "\n",
      "epoch:     7|00:58| train_loss:  2.5161 | valid_loss:  2.5032\n",
      "\n",
      "Evaluation diagnostics:\n",
      "expressions correct: 2\n",
      "chars correct per expression: 19.159329\n",
      "char correct rate: 0.227780\n",
      "chars per expression: 84.113208\n",
      "\n",
      "epoch:     8|00:57| train_loss:  2.4977 | valid_loss:  2.4762\n",
      "\n",
      "Evaluation diagnostics:\n",
      "expressions correct: 6\n",
      "chars correct per expression: 20.984340\n",
      "char correct rate: 0.215846\n",
      "chars per expression: 97.219239\n",
      "\n",
      "epoch:     9|00:58| train_loss:  2.5074 | valid_loss:  2.4184\n",
      "\n",
      "Evaluation diagnostics:\n",
      "expressions correct: 0\n",
      "chars correct per expression: 20.752759\n",
      "char correct rate: 0.213175\n",
      "chars per expression: 97.350993\n",
      "\n",
      "epoch:    10|01:00| train_loss:  2.5427 | valid_loss:  2.4645\n",
      "\n",
      "Evaluation diagnostics:\n",
      "expressions correct: 2\n",
      "chars correct per expression: 20.237991\n",
      "char correct rate: 0.222705\n",
      "chars per expression: 90.873362\n",
      "\n",
      "epoch:    11|01:03| train_loss:  2.5388 | valid_loss:  2.4081\n",
      "\n",
      "Evaluation diagnostics:\n",
      "expressions correct: 1\n",
      "chars correct per expression: 19.889381\n",
      "char correct rate: 0.210544\n",
      "chars per expression: 94.466814\n",
      "\n",
      "epoch:    12|00:57| train_loss:  2.5038 | valid_loss:  2.4995\n",
      "\n",
      "Evaluation diagnostics:\n",
      "expressions correct: 2\n",
      "chars correct per expression: 20.113734\n",
      "char correct rate: 0.216047\n",
      "chars per expression: 93.098712\n",
      "\n",
      "epoch:    13|00:58| train_loss:  2.5103 | valid_loss:  2.5031\n",
      "\n",
      "Evaluation diagnostics:\n",
      "expressions correct: 1\n",
      "chars correct per expression: 21.266667\n",
      "char correct rate: 0.207930\n",
      "chars per expression: 102.278161\n",
      "\n",
      "epoch:    14|00:58| train_loss:  2.4996 | valid_loss:  2.5063\n",
      "\n",
      "Evaluation diagnostics:\n",
      "expressions correct: 2\n",
      "chars correct per expression: 20.787879\n",
      "char correct rate: 0.215283\n",
      "chars per expression: 96.560606\n",
      "\n",
      "epoch:    15|00:58| train_loss:  2.5159 | valid_loss:  2.4828"
     ]
    }
   ],
   "source": [
    "# training!\n",
    "n_epochs = 1100 # number of times through training data\n",
    "\n",
    "model.load_state_dict(torch.load(model_file))\n",
    "\n",
    "train_me_record_loss(model_file, loss_file, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "executionInfo": {
     "elapsed": 2382,
     "status": "ok",
     "timestamp": 1626529656054,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "XHWiL-3XR_y5",
    "outputId": "e8833853-ac90-4128-df7e-ca7a80b2a65c"
   },
   "outputs": [],
   "source": [
    "# update fonts\n",
    "FONTSIZE = 16\n",
    "font = {'family' : 'serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : FONTSIZE}\n",
    "mp.rc('font', **font)\n",
    "mp.rc('xtick', labelsize='x-small')\n",
    "mp.rc('ytick', labelsize='x-small')\n",
    "mp.rc('text', usetex=True)\n",
    "\n",
    "def plot_losses(loss_file, plot_file='fig_losses.pdf', \n",
    "                xlimits=None, \n",
    "                ylimits=None, \n",
    "                hidden_size=HIDDEN_SIZE, \n",
    "                num_layers=NUM_LAYERS, \n",
    "                encoder_embed_size=ENCODER_EMBED_SIZE, \n",
    "                encoder_dropout=ENCODER_DROPOUT,                 \n",
    "                decoder_embed_size=DECODER_EMBED_SIZE, \n",
    "                decoder_dropout=DECODER_DROPOUT,\n",
    "                num_parameters=NUM_PARAMETERS):\n",
    "    \n",
    "    losses       = [a.split() for a in open(loss_file).readlines()]\n",
    "    train_losses = [float(z)  for z, _ in losses]\n",
    "    valid_losses = [float(z)  for _, z in losses]\n",
    "\n",
    "    # do a bit of smoothing\n",
    "    # see: https://danielmuellerkomorowska.com/2020/06/02/\n",
    "    #      smoothing-data-by-rolling-average-with-numpy/\n",
    "    kernel_size  = 10\n",
    "    kernel = np.ones(kernel_size) / kernel_size\n",
    "    train_losses = np.convolve(train_losses, kernel, mode='same')\n",
    "    valid_losses = np.convolve(valid_losses, kernel, mode='same')\n",
    "    epochs       = np.arange(len(train_losses))\n",
    "   \n",
    "    # plot loss curves\n",
    "    # set up an empty figure\n",
    "    fig = plt.figure(figsize=(7, 4))\n",
    "    \n",
    "    # add a subplot to it\n",
    "    nrows, ncols, index = 1,1,1\n",
    "    ax  = fig.add_subplot(nrows, ncols, index)\n",
    "    \n",
    "    ax.set_xlabel('epoch', fontsize=16)\n",
    "    ax.set_ylabel('$\\overline{loss}$', fontsize=16) \n",
    "    \n",
    "    if xlimits: ax.set_xlim(xlimits)\n",
    "    if ylimits: ax.set_ylim(ylimits)\n",
    "    \n",
    "    ax.plot(epochs, train_losses, c='red',  label='training')\n",
    "    ax.plot(epochs, valid_losses, c='blue', label='validation')\n",
    "    \n",
    "    #ax.grid(True, which=\"both\", linestyle='-')\n",
    "    ax.legend()\n",
    "\n",
    "    # annotate\n",
    "    xmin, xmax = ax.get_xlim()\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    ystep = (ymax - ymin)/12\n",
    "    xpos  = xmin + 0.33*(xmax-xmin)\n",
    "    ypos  = ymax - ystep\n",
    "    fsize = 14\n",
    "\n",
    "    ax.text(xpos, ypos, 'hidden\\_size: %4d' % hidden_size,\n",
    "            fontsize=fsize); ypos -= ystep\n",
    "    \n",
    "    ax.text(xpos, ypos, 'num\\_layers: %4d' % num_layers, \n",
    "            fontsize=fsize); ypos -= ystep \n",
    "\n",
    "    ax.text(xpos, ypos, 'encoder\\_embed\\_size: %4d'%encoder_embed_size,\n",
    "            fontsize=fsize); ypos -= ystep \n",
    "    \n",
    "    ax.text(xpos, ypos, 'encoder\\_dropout: %4.1f' % encoder_dropout, \n",
    "            fontsize=fsize); ypos -= ystep \n",
    "        \n",
    "    ax.text(xpos, ypos, 'decoder\\_embed\\_size: %4d'%decoder_embed_size,\n",
    "            fontsize=fsize); ypos -= ystep \n",
    "        \n",
    "    ax.text(xpos, ypos, 'decoder\\_dropout: %4.1f' % decoder_dropout, \n",
    "            fontsize=fsize); ypos -= ystep \n",
    "        \n",
    "    ax.text(xpos, ypos, 'number of free parameters: %7d'%num_parameters, \n",
    "            fontsize=fsize); ypos -= ystep \n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.savefig(plot_file)  \n",
    "    plt.show()\n",
    "    \n",
    "plot_file   = '%s/seq2seq_losses_%2.2d_fig.pdf' % (BASE, model_count)\n",
    "plot_losses(loss_file, plot_file=plot_file,\n",
    "            xlimits=(0, 3500), ylimits=(2.2, 3.2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute average loss over full test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4aIIAQ7AVtya"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_file, \n",
    "                                 map_location=torch.device('cpu')))\n",
    "\n",
    "test_loss = evaluate(model, test_dataloader, \n",
    "                     criterion, DEVICE,  \n",
    "                     num_samples=-1, \n",
    "                     sample=False)\n",
    "print('test loss: %7.4f' % test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPpN1AZeVtya"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "seq2seq_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 - AI",
   "language": "python",
   "name": "python3-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
