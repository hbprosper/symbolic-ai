{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fELG9vDJuJdc"
   },
   "source": [
    "# seq2seq: Data Preparation\n",
    "\n",
    "Quarks To Cosmos with AI Virtual Conference: July 12-16, 2021, Carnegie Mellon University\n",
    "\n",
    "## Contributors\n",
    "\n",
    "Abdulhakim Alnuqaydan, Ali Kadhim, Sergei Gleyzer, Harrison Prosper\n",
    "\n",
    "## Hackathon Contributors\n",
    "\n",
    "Andrew Roberts, Jessica Howard, Samuel Hori, Arvind Balasubramanian, Xiaosheng Zhao, Michael Andrews\n",
    "\n",
    "July 2021\n",
    "\n",
    "## Description\n",
    "\n",
    "Use an encoder/decoder model built using LSTMs to map symbolic mathematical expressions $f(x)$ to their Taylor series expansions to ${\\cal O}(x^5)$.\n",
    "\n",
    "We've heavily borrowed from Charon Guo's excellent tutorial at:\n",
    "\n",
    "https://charon.me/posts/pytorch/pytorch_seq2seq_1/\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "This notebook performs the following tasks:\n",
    "  1. Read the sequence pairs from __data/seq2seq_data.txt__.\n",
    "  1. Exclude\n",
    "     1. sequences with complex numbers and with Taylor series expansions longer than 1000 characters.\n",
    "     1. trivial source expressions.\n",
    "  1. Write the filtered sequences to __data/seq2seq_data_count.txt__, where count is either 10,000 or 60,000 sequences.\n",
    "  1. Write out __seq2sequtil.py__.\n",
    "  1. Read filtered data and delimit source (i.e, input) and target (i.e., output) sequences with a tab and newline at the start and end of each sequence, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GwFXx5YluJde"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sympy as sp\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# symbolic mathematics\n",
    "from sympy import exp, \\\n",
    "    cos, sin, tan, \\\n",
    "    cosh, sinh, tanh, ln, log, E\n",
    "x = sp.Symbol('x')\n",
    "\n",
    "from IPython.display import display\n",
    "    \n",
    "# enable pretty printing of equations\n",
    "sp.init_printing(use_latex='mathjax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[ \\right]$"
      ],
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reproducing a subtle bug!\n",
    "data = [['a(4*x)','b(x)'], ['1(exp)','2'], ['3(x)','4']]\n",
    "non_trivial = re.compile(r'(a|3)'\\\n",
    "                         '[(].*\\bx\\b')\n",
    "# eliminate expressions that do not involve x, exp, cos etc.\n",
    "data = filter(lambda d: len(non_trivial.findall(d[0])) > 0, data)\n",
    "data = list(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### include non-trival expressions involving 'x' only:  \n",
    "1. $(f(x)\\pm g(x))^{h(x)}$\n",
    "2. $\\frac{h(x)}{(f(x)\\pm g(x))}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1(exp(x))', '2'],\n",
       " ['sin(5*x)', '5'],\n",
       " ['(1+x)**6', '4'],\n",
       " ['(1+x)/(2-x)', '4']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [['a(4*x)','b(x)'], ['1(exp(x))','2'], ['3(x+5)','4'],\n",
    "        ['sin(5*x)','5'],['x**5','x*5'],['(1+x)**6','4'],\n",
    "        ['(1+x)/(2-x)','4'],['(1+x)/(2+1)','4']]\n",
    "\n",
    "non_trivial = re.compile(r'(exp|cos|sin|tan|ln|log|cosh|sinh|tanh)[(].*\\bx\\b|'\\\n",
    "                         r'[(].*[+-].*\\bx\\b.*[)][*][*]|'\\\n",
    "                         r'.*/[(].*[+-]\\bx\\b.*')\n",
    "\n",
    "data = filter(lambda d: len(non_trivial.findall(d[0])) > 0, data)\n",
    "data = list(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output file: data/seq2seq_data_100.txt\n",
      "CPU times: user 59.5 s, sys: 54.8 ms, total: 59.5 s\n",
      "Wall time: 59.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "of_order    = re.compile(' [+] O[(]x[*][*]5.*[)]')\n",
    "# Please note that breaking a raw string does not propagate its \n",
    "# \"rawness\" across the break :(\n",
    "non_trivial = re.compile(r'(exp|cos|sin|tan|ln|log|cosh|sinh|tanh)'\\\n",
    "                         r'[(].*\\bx\\b')\n",
    "'''\n",
    "non_trivial = re.compile(r'(exp|cos|sin|tan|ln|log|cosh|sinh|tanh)[(].*\\bx\\b|'\\\n",
    "                         r'[(].*[+-].*\\bx\\b.*[)][*][*]|'\\\n",
    "                         r'.*/[(].*[+-]\\bx\\b.*')\n",
    "'''\n",
    "add_count   = re.compile('_data')\n",
    "\n",
    "def filterData(inpfile='data/seq2seq_data.txt',\n",
    "               num_seq=60000,  # number of filtered sequence pairs\n",
    "               min_len=5,      # minimum length of a sequence\n",
    "               max_len=1000):  # maximum length of a sequence\n",
    "    \n",
    "    data = open(inpfile).readlines()\n",
    "    \n",
    "    # eliminate expressions involving complex numbers\n",
    "    data = filter(lambda d: d.find('I') < 0, data)\n",
    "    data = list(data)\n",
    "\n",
    "    # strip away O(...) (of order..)\n",
    "    data = [of_order.sub('', d) for d in data]\n",
    " \n",
    "    # split pairs at tab\n",
    "    data = [ d.split('\\t') for d in data ]\n",
    "    #print(data[:5])\n",
    "    \n",
    "    # keep source expressions that involve exp, cos, etc.\n",
    "    # that is, eliminate trivial expressions.\n",
    "    data = filter(lambda d: len(non_trivial.findall(d[0])) > 0, data)\n",
    "    data = list(data)\n",
    " \n",
    "    # keep expressions that are >= min_len characters long\n",
    "    data = filter(lambda d: \n",
    "                  (len(d[0]) >= min_len) and (len(d[1]) >= min_len),\n",
    "                  data)\n",
    "    data = list(data)\n",
    "                  \n",
    "    # keep expressions that are <= max_len characters long\n",
    "    data = filter(lambda d: \n",
    "                  (len(d[0]) <= max_len) and (len(d[1]) <= max_len), \n",
    "                  data)\n",
    "    data = list(data)\n",
    "    \n",
    "    N = min(num_seq, len(data))\n",
    "    #print(len(data))\n",
    "    \n",
    "    # simplify the expressions #### Xiaosheng\n",
    "    # Cancel common factors in the numerator and denominator.\n",
    "    #data=[[''.join(str(sp.cancel(d[0])).split()),str(sp.cancel(d[1]))+'\\n'] for d in data[:N]] #2.6s\n",
    "    # Simplify only the trigonometric parts of the expression.\n",
    "    #data=[[''.join(str(sp.trigsimp(d[0])).split()),str(sp.trigsimp(d[1]))+'\\n'] for d in data[:N]] #23.6s\n",
    "    # Simplify an expression.\n",
    "    data=[[''.join(str(sp.simplify(d[0])).split()),str(sp.simplify(d[1]))+'\\n'] for d in data[:N]] #36.2s\n",
    "    \n",
    "    outfile = add_count.sub('_data_%d' % N, inpfile)\n",
    "    print('output file:', outfile)\n",
    "    \n",
    "    data = ['\\t'.join(d) for d in data]\n",
    "    open(outfile, 'w').writelines(data[:N])\n",
    "    \n",
    "filterData(num_seq=100)\n",
    "#filterData(num_seq=10000)\n",
    "#filterData(num_seq=60000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map sequences to lists of indices\n",
    "\n",
    "  1. Split data into a train, validation, and test set.\n",
    "  1. Create a token (i.e., a character) to index map from training data.\n",
    "  1. Map sequences to arrays of indices.\n",
    "  1. Implement custom DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting seq2sequtil.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile seq2sequtil.py\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from IPython.display import display\n",
    "\n",
    "# symbolic symbols\n",
    "from sympy import Symbol, exp, \\\n",
    "    cos, sin, tan, \\\n",
    "    cosh, sinh, tanh, ln, log, E\n",
    "x = Symbol('x')\n",
    "\n",
    "class Seq2SeqDataPreparer:\n",
    "    '''\n",
    "    This class maps the source (i.e., input) and target (i.e, output) \n",
    "    sequences of characters into sequences of indices. The source data \n",
    "    are split into x_train, x_valid, and x_test sets and similarly for \n",
    "    the target data.\n",
    "    \n",
    "    Create a data preparer using\n",
    "    \n",
    "    dd = Seq2SeqDataPreparer(X, Y, fractions)\n",
    "    \n",
    "    where,\n",
    "\n",
    "      fractions:    a 2-tuple containing the three-way split of data.\n",
    "                    e.g.: (50/60, 55/60) means split the data as follows\n",
    "                    (50000, 5000, 5000)\n",
    "    '''\n",
    "    def __init__(self, X, Y,\n",
    "                 fractions=[50/60, 55/60]): \n",
    "        \n",
    "        self.fractions = fractions\n",
    "        \n",
    "        # Get maximum sequence length for input expressions\n",
    "        #self.x_max_seq_len =  max([len(z) for z in X])\n",
    "        \n",
    "        self.x_max_seq_len =  max([self.split_expr(z)[1] for z in X])\n",
    "        \n",
    "        # Get maximum sequence length for target expressions\n",
    "        #self.y_max_seq_len =  max([len(z) for z in Y])\n",
    "        \n",
    "        self.y_max_seq_len =  max([self.split_expr(z)[1] for z in Y])\n",
    "        \n",
    "        # get length of splits into train, valid, test\n",
    "        N = int(len(X)*fractions[0])\n",
    "        M = int(len(X)*fractions[1])\n",
    "        \n",
    "        # Create token to index map for source sequences\n",
    "        t = self.token_tofrom_index(X[:N])\n",
    "        self.x_token2index, self.x_index2token = t\n",
    "        \n",
    "        # Create token to index map for target sequences\n",
    "        t = self.token_tofrom_index(Y[:N])\n",
    "        self.y_token2index,self.y_index2token = t\n",
    "        \n",
    "        # Structure data into a list of blocks, where each block\n",
    "        # comprises a tuple (x_data, y_data) whose elements have\n",
    "        #   x_data.shape: (x_seq_len, batch_size)\n",
    "        #   y_data.shape: (y_seq_len, batch_size)\n",
    "        #\n",
    "        # The sequence and batch sizes can vary from block to block.\n",
    "        \n",
    "        self.train_data, self.n_train = self.code_data(X[:N], Y[:N])         \n",
    "        self.valid_data, self.n_valid = self.code_data(X[N:M],Y[N:M])\n",
    "        self.test_data,  self.n_test  = self.code_data(X[M:], Y[M:])\n",
    "\n",
    "    def __del__(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        n  = 0\n",
    "        n += self.n_train\n",
    "        n += self.n_valid\n",
    "        n += self.n_test\n",
    "        return n\n",
    "    \n",
    "    def __str__(self):\n",
    "        s  = ''\n",
    "        s += 'number of seq-pairs (train): %8d\\n' % self.n_train\n",
    "        s += 'number of seq-pairs (valid): %8d\\n' % self.n_valid\n",
    "        s += 'number of seq-pairs (test):  %8d\\n' % self.n_test\n",
    "        s += '\\n'\n",
    "        s += 'number of source tokens:     %8d\\n' % \\\n",
    "        len(self.x_token2index)\n",
    "        s += 'max source sequence length:  %8d\\n' % \\\n",
    "        self.x_max_seq_len\n",
    "        \n",
    "        try:\n",
    "            s += '\\n'\n",
    "            s += 'number of target tokens:     %8d\\n' % \\\n",
    "            len(self.y_token2index)\n",
    "            s += 'max target sequence length:  %8d' % \\\n",
    "            self.y_max_seq_len\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return s\n",
    "         \n",
    "    def num_tokens(self, which='source'):\n",
    "        if which[0] in ['s', 'i']:\n",
    "            return len(self.x_token2index)\n",
    "        else:\n",
    "            return len(self.y_token2index)\n",
    "    \n",
    "    def max_seq_len(self, which='source'):\n",
    "        if which[0] in ['s', 'i']:\n",
    "            return self.x_max_seq_len\n",
    "        else:\n",
    "            return self.y_max_seq_len\n",
    "        \n",
    "    def decode(self, indices):\n",
    "        # map list of indices to a list of tokens\n",
    "        return ''.join([self.y_index2token[i] for i in indices])\n",
    "\n",
    "    #def token_tofrom_index(self, expressions):\n",
    "    #    chars = set()\n",
    "    #    chars.add(' ')  # for padding\n",
    "    #    chars.add('?')  # for unknown characters\n",
    "    #    for expression in expressions:\n",
    "    #        for char in expression:\n",
    "    #            chars.add(char)\n",
    "    #    chars = sorted(list(chars))\n",
    "    #    \n",
    "    #    char2index = dict([(char, i) for i, char in enumerate(chars)])\n",
    "    #    index2char = dict([(i, char) for i, char in enumerate(chars)])\n",
    "    #    return (char2index, index2char)\n",
    "    \n",
    "    def token_tofrom_index(self,expressions,special_expressions_str=['exp','cos','sin','tan','ln','log','cosh','sinh','tanh','**']):\n",
    "        chars = set()\n",
    "        chars.add(' ')  # for padding\n",
    "        chars.add('?')  # for unknown characters\n",
    "        for expression in expressions:\n",
    "            # Map special function characters into single index eg: sin --> 23\n",
    "            regex_str = '|'.join(special_expressions_str)\n",
    "            regex_str = regex_str.replace('**','\\*\\*')\n",
    "            regex = re.compile(r'\\b('+regex_str+r')\\b')\n",
    "            for spec_exp in special_expressions_str:\n",
    "                chars.add(spec_exp)\n",
    "            new_expression = regex.sub('',expression)\n",
    "            for n_exp in new_expression:\n",
    "                chars.add(n_exp)\n",
    "        chars = sorted(list(chars))\n",
    "        \n",
    "        char2index = dict([(char, i) for i, char in enumerate(chars)])\n",
    "        index2char = dict([(i, char) for i, char in enumerate(chars)])\n",
    "        return (char2index, index2char)\n",
    "       \n",
    "    def get_block_indices(self, X, Y):\n",
    "        # X, and Y are just arrays of strings.\n",
    "        #\n",
    "        # 1. Following Michael Andrews' suggestion double sort \n",
    "        #    expressions, first with targets then sources. But, also\n",
    "        #    note the ordinal values \"i\" of the expressions in X, Y.\n",
    "        \n",
    "        #sizes = [(len(a), len(b), i) \n",
    "        #         for i, (a, b) in enumerate(zip(Y, X))]\n",
    "        \n",
    "        sizes = [(self.split_expr(a)[1], self.split_expr(b)[1], i) ##### Arvind\n",
    "                 for i, (a, b) in enumerate(zip(Y, X))]\n",
    "        sizes.sort()\n",
    "  \n",
    "        # 2. Find ordinal values (indices) of all expression pairs \n",
    "        #    for which the sources are the same length and the\n",
    "        #    targets are the same length. In general, the sources and\n",
    "        #    targets differ in length.\n",
    "     \n",
    "        block_indices = []\n",
    "        n, m, i  = sizes[0] # n, m, i = len(target), len(source), index\n",
    "        previous = (n, m)\n",
    "        indices  = [i] # cache index of first expression\n",
    "        \n",
    "        for n, m, i in sizes[1:]: # skip first expression\n",
    "            \n",
    "            size = (n, m)\n",
    "            \n",
    "            if size == previous:\n",
    "                indices.append(i) # cache index of expression\n",
    "            else:\n",
    "                # found a new boundary, so save previous \n",
    "                # set of indices...\n",
    "                block_indices.append(indices)\n",
    "                \n",
    "                # ...and start a new list of indices\n",
    "                indices = [i]\n",
    "\n",
    "            previous = size\n",
    "            \n",
    "        # cache expression indices of last block\n",
    "        block_indices.append(indices)\n",
    "        \n",
    "        return block_indices\n",
    "    \n",
    "    \n",
    "    def make_block(self, expressions, indices, token2index, unknown):\n",
    "        \n",
    "        # batch size of current block\n",
    "        batch_size = len(indices)\n",
    "        \n",
    "        # By construction, all expressions of a block have \n",
    "        # the same length, so can use the length of first expression\n",
    "        #seq_len = len(expressions[indices[0]])\n",
    "        \n",
    "        seq_len = self.split_expr(expressions[indices[0]])[1] ### Arvind\n",
    "        \n",
    "        # Create an empty block of correct shape and size\n",
    "        data    = np.zeros((seq_len, batch_size), dtype='long')\n",
    "        #print('seq_len, batch_size: (%d, %d)' % (seq_len, batch_size))\n",
    "        \n",
    "        # loop over expressions for current block\n",
    "        # m: ordinal value of expression in current block\n",
    "        # k: ordinal value of expression in original list of expressions\n",
    "        # n: ordinal value of character in a given expression\n",
    "        \n",
    "        for m, k in enumerate(indices):\n",
    "            \n",
    "            expr = expressions[k]\n",
    "            \n",
    "            #print('%5d expr[%d] | %s |' % (m, k, expr[1:-1]))\n",
    "            \n",
    "            # copy coded characters to 2D arrays\n",
    "            \n",
    "            reduced_expr = self.split_expr(expr)[0] #### Arvind\n",
    "            \n",
    "            #for n, char in enumerate(expr):\n",
    "            for n, char in enumerate(reduced_expr): ###Arvind\n",
    "                #print('\\t\\t(n, m): (%d, %d)' % (n, m))\n",
    "                try:\n",
    "                    data[n, m] = token2index[char]\n",
    "                except:\n",
    "                    data[n, m] = unknown\n",
    "                    \n",
    "        return data\n",
    "    \n",
    "    def code_data(self, X, Y):\n",
    "        # Implement Arvind's idea\n",
    "        \n",
    "        # X, Y consist of delimited strings: \n",
    "        #   \\tab<characters\\newline\n",
    "        \n",
    "        # loop over sequence pairs and convert them to sequences\n",
    "        # of integers using the two token2index maps\n",
    "      \n",
    "        x_space   = self.x_token2index[' ']\n",
    "        x_unknown = self.x_token2index['?']\n",
    "        \n",
    "        y_space   = self.y_token2index[' ']\n",
    "        y_unknown = self.y_token2index['?']\n",
    " \n",
    "        # 1. Get blocks containing sequences of the same length.\n",
    "        \n",
    "        block_indices = self.get_block_indices(X, Y)\n",
    "        \n",
    "        # 2. Loop over the indices associated with each block of coded\n",
    "        #    sequences. The indices are the ordinal values of the\n",
    "        #    sequence pairs X and Y.\n",
    "        \n",
    "        blocks = []\n",
    "        n_data = 0\n",
    "       \n",
    "        for indices in block_indices:\n",
    "\n",
    "            x_data = self.make_block(X, indices, \n",
    "                                     self.x_token2index, x_unknown)\n",
    " \n",
    "            y_data = self.make_block(Y, indices, \n",
    "                                     self.y_token2index, y_unknown)\n",
    "\n",
    "            blocks.append((x_data, y_data))\n",
    "            n = len(indices)\n",
    "            n_data += n\n",
    "        \n",
    "        assert n_data == len(X)\n",
    "        \n",
    "        return blocks, n_data\n",
    "    \n",
    "    def split_expr(self, expression, individual_tokens=['exp','cos','sin','tan','ln','log','cosh','sinh','tanh','**'\\\n",
    "                           ,'1','2','3','4','5','6','7','8','9','0','x','\\n','\\t']):\n",
    "    \n",
    "    #Returns a split expression (into individual character groups that are to be passed on\n",
    "    #to the token_tofrom_index function to map to indices). Also returns length of final\n",
    "    #expression (counting len(sin) as 1) to be used for caluclating len of expressions \n",
    "    #during block generation.\n",
    "\n",
    "    \n",
    "        regex_str = '|'.join(individual_tokens)\n",
    "        regex_str = regex_str.replace('**','\\*\\*')\n",
    "        regex = re.compile(r'('+regex_str+r'|[()+-/*])')\n",
    "        reduced_expression_chars = re.split(regex,expression)\n",
    "        while '' in reduced_expression_chars:\n",
    "            reduced_expression_chars.remove('')\n",
    "        return reduced_expression_chars, len(reduced_expression_chars)\n",
    "    \n",
    "class Seq2SeqDataLoader:\n",
    "    '''\n",
    "    dataloader = Seq2seqDataLoader(dataset, device, sample=True)    \n",
    "    '''\n",
    "    def __init__(self, dataset, device, sample=True):\n",
    "        self.dataset = dataset\n",
    "        self.device  = device\n",
    "        self.sample  = sample  \n",
    "        self.init()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        \n",
    "        # increment iteration counter\n",
    "        self.count += 1\n",
    "        \n",
    "        if self.count <= self.max_count:\n",
    "            \n",
    "            # 1. randomly pick a block or return blocks in order.\n",
    "            if self.sample:\n",
    "                k = np.random.randint(len(self.dataset))\n",
    "            else:\n",
    "                k = self.count-1 # must subtract one!\n",
    "            \n",
    "            # 2. create tensors directly on the device of interest\n",
    "            X = torch.tensor(self.dataset[k][0], \n",
    "                             device=self.device)\n",
    "            \n",
    "            Y = torch.tensor(self.dataset[k][1], \n",
    "                             device=self.device)\n",
    "        \n",
    "            # shape of X and Y: (seq_len, batch_size)\n",
    "            return X, Y\n",
    "        else:\n",
    "            self.count = 0\n",
    "            raise StopIteration\n",
    "            \n",
    "    def init(self, max_count=0, sample=True):\n",
    "        n_data = len(self.dataset)\n",
    "        self.max_count = n_data if max_count < 1 else min(max_count, \n",
    "                                                          n_data)\n",
    "        self.sample= sample\n",
    "        self.count = 0\n",
    "        \n",
    "# Delimit each sequence in filtered sequences\n",
    "# The start of sequence (SOS) and end of sequence (EOS) \n",
    "# tokens are \"\\t\" and \"\\n\", respectively.\n",
    "\n",
    "def loadData(inpfile):\n",
    "    # format of data:\n",
    "    # input expression<tab>target expression<newline>\n",
    "    data = [a.split('\\t') for a in open(inpfile).readlines()]\n",
    "    \n",
    "    X, Y = [], []\n",
    "    for i, (x, y) in enumerate(data):\n",
    "        X.append('\\t%s\\n' % x)\n",
    "        # get rid of spaces in target sequence\n",
    "        y = ''.join(y.split())\n",
    "        Y.append('\\t%s\\n' % y)\n",
    "        \n",
    "    print('Example source:')\n",
    "    print(X[-1])\n",
    "    pprint(X[-1])\n",
    "    print('Example target:')\n",
    "    print(Y[-1])\n",
    "    pprint(Y[-1])\n",
    "\n",
    "    return (X, Y)\n",
    "\n",
    "def loadData_simplify(inpfile):\n",
    "    # format of data:  ##### Xiaosheng\n",
    "    # input expression<tab>target expression<newline>\n",
    "    data = [a.split('\\t') for a in open(inpfile).readlines()]\n",
    "    \n",
    "    X, Y = [], []\n",
    "    for i, (x, y) in enumerate(data):\n",
    "        X.append('\\t%s\\n' % x)\n",
    "        # get rid of spaces in target sequence\n",
    "        y = ''.join(y.split())\n",
    "        Y.append('\\t%s\\n' % y)\n",
    "        \n",
    "    for ii in range(-10,-1):\n",
    "        #simplify the expressions\n",
    "        X_simp=sp.simplify(X[ii])\n",
    "        targets_simp=sp.simplify(Y[ii])\n",
    "\n",
    "        #print the sources\n",
    "        print('Example source:')\n",
    "        print (X[ii])\n",
    "        pprint (X[ii])\n",
    "        print('\\033[1;34m'+'Simplified example source:'+'\\033[0m')\n",
    "        print (X_simp)\n",
    "        pprint (str(X_simp))\n",
    "\n",
    "        #print the targets\n",
    "        print('Example target:')\n",
    "        print (Y[ii])\n",
    "        pprint (Y[ii])\n",
    "        print('\\033[1;34m'+ 'Simplified example target:'+'\\033[0m')\n",
    "        print (targets_simp)\n",
    "        pprint (str(targets_simp))\n",
    "        \n",
    "        print ('\\033[1;31m'+'*'*100+'\\033[0m')\n",
    "        \n",
    "    return (X, Y)\n",
    "\n",
    "def pprint(expr):\n",
    "    display(eval(expr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display a few sequence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example source:\n",
      "\tcosh(9*x-7)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\cosh{\\left(9 x - 7 \\right)}$"
      ],
      "text/plain": [
       "cosh(9⋅x - 7)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example target:\n",
      "\t2187*x**4*cosh(7)/8-243*x**3*sinh(7)/2+81*x**2*cosh(7)/2-9*x*sinh(7)+cosh(7)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{2187 x^{4} \\cosh{\\left(7 \\right)}}{8} - \\frac{243 x^{3} \\sinh{\\left(7 \\right)}}{2} + \\frac{81 x^{2} \\cosh{\\left(7 \\right)}}{2} - 9 x \\sinh{\\left(7 \\right)} + \\cosh{\\left(7 \\right)}$"
      ],
      "text/plain": [
       "      4                3               2                                \n",
       "2187⋅x ⋅cosh(7)   243⋅x ⋅sinh(7)   81⋅x ⋅cosh(7)                        \n",
       "─────────────── - ────────────── + ───────────── - 9⋅x⋅sinh(7) + cosh(7)\n",
       "       8                2                2                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seq2sequtil as sq\n",
    "import importlib\n",
    "importlib.reload(sq)\n",
    "#inputs, targets = sq.loadData('data/seq2seq_data_10000.txt')\n",
    "inputs, targets = sq.loadData('data/seq2seq_data_100.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data preparer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of seq-pairs (train):       80\n",
      "number of seq-pairs (valid):       10\n",
      "number of seq-pairs (test):        10\n",
      "\n",
      "number of source tokens:           31\n",
      "max source sequence length:        74\n",
      "\n",
      "number of target tokens:           32\n",
      "max target sequence length:      3881\n"
     ]
    }
   ],
   "source": [
    "fractions=[8/10, 9/10]\n",
    "db = sq.Seq2SeqDataPreparer(inputs, targets, fractions)\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\t', 1: '\\n', 2: ' ', 3: '(', 4: ')', 5: '*', 6: '**', 7: '+', 8: '-', 9: '/', 10: '0', 11: '1', 12: '2', 13: '3', 14: '4', 15: '5', 16: '6', 17: '7', 18: '8', 19: '9', 20: '?', 21: 'cos', 22: 'cosh', 23: 'exp', 24: 'ln', 25: 'log', 26: 'sin', 27: 'sinh', 28: 'tan', 29: 'tanh', 30: 'x'}\n",
      "{0: '\\t', 1: '\\n', 2: ' ', 3: '(', 4: ')', 5: '*', 6: '**', 7: '+', 8: '-', 9: '/', 10: '0', 11: '1', 12: '2', 13: '3', 14: '4', 15: '5', 16: '6', 17: '7', 18: '8', 19: '9', 20: '?', 21: 'E', 22: 'cos', 23: 'cosh', 24: 'exp', 25: 'ln', 26: 'log', 27: 'sin', 28: 'sinh', 29: 'tan', 30: 'tanh', 31: 'x'}\n",
      "[ 0 24  3  8 11  4  8 15  5 31  5 24  3  8 11  4  7 12 15  5 31  6 12  5\n",
      " 24  3  8 11  4  9 12  8 11 12 15  5 31  6 13  5 24  3  8 11  4  9 16  7\n",
      " 16 12 15  5 31  6 14  5 24  3  8 11  4  9 12 14  1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\texp(-1)-5*x*exp(-1)+25*x**2*exp(-1)/2-125*x**3*exp(-1)/6+625*x**4*exp(-1)/24\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(db.x_index2token)\n",
    "print(db.y_index2token)\n",
    "test_expression = '\\texp(-1)-5*x*exp(-1)+25*x**2*exp(-1)/2-125*x**3*exp(-1)/6+625*x**4*exp(-1)/24\\n'\n",
    "\n",
    "# Code the data\n",
    "data = db.make_block([test_expression],[0],db.y_token2index,db.y_token2index['?'])\n",
    "print(data[:,0])\n",
    "# Test by decoding the coded data\n",
    "db.decode(data[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block\tX.shape             \tY.shape             \n",
      "    0\ttorch.Size([21, 1]) \ttorch.Size([71, 1]) \n",
      " 1000\ttorch.Size([27, 1]) \ttorch.Size([27, 1]) \n",
      " 2000\ttorch.Size([11, 1]) \ttorch.Size([50, 1]) \n",
      " 3000\ttorch.Size([36, 1]) \ttorch.Size([183, 1])\n",
      " 4000\ttorch.Size([24, 1]) \ttorch.Size([199, 1])\n",
      "\n",
      "number of blocks: 4058\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_loader = sq.Seq2SeqDataLoader(db.train_data, device)\n",
    "\n",
    "n = 0\n",
    "print('%5s\\t%-20s\\t%-20s' % ('block', 'X.shape', 'Y.shape'))\n",
    "for i, (X, Y) in enumerate(train_loader):\n",
    "    if i % 1000 == 0:\n",
    "        print('%5d\\t%-20s\\t%-20s' % (i, X.shape, Y.shape))\n",
    "    n += 1\n",
    "print(\"\\nnumber of blocks: %d\" % n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "seq2seq_data_generator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
